{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0TppvsITQEW"
      },
      "source": [
        "# Tutorial 2: analyzing StreamSBI\n",
        "By: Rutong Pei\n",
        "\n",
        "In this tutorial, we will walk through how to analyze the input and output of SBI model by examples, in particular making plots of streams and posteriors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6u88fgmSy2tt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "import corner\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from tqdm import tqdm\n",
        "from scipy.interpolate import LSQUnivariateSpline\n",
        "\n",
        "import datasets\n",
        "from models import regressor, infer_utils\n",
        "from datasets import io_utils, preprocess_utils\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6E4PR2uIwHK"
      },
      "source": [
        "# Analyze the input streams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xiw8WJ5Ul9c"
      },
      "source": [
        "In order to get particle and binned simulation data, we first specify the dataset we are using, and labels associated with the run. Specifically we cut `phi1` range to be `[-20, 12]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsYMBl0sUbOt"
      },
      "outputs": [],
      "source": [
        "#get particle and binned simulation data\n",
        "data_name = '/2params-n1000'\n",
        "raw_data_dir = f'/pscratch/sd/t/tvnguyen/stream_sbi/datasets/{data_name}'\n",
        "data_labels = ['log_M_sat', 'vz']\n",
        "num_bins = 10 #use 10 to 50\n",
        "phi1_min = -20\n",
        "phi1_max = 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUbhiNQ-1A14"
      },
      "source": [
        "This code block reads the raw simulation data from a given directory, which contain `phi1, phi2, pm1, pm2, vr, distance` of each particle. You can index into different streams. For example, `raw[0]` will return the first stream in this dataset (`raw[0].shape` = 6 * #particles in stream) and `raw[0][1]` will return `phi2` of all particles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiUGaIvf2goh"
      },
      "outputs": [],
      "source": [
        "def read_raw_dataset(data_dir, labels, num_datasets=1):\n",
        "    \"\"\" Read raw data\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_dir : str\n",
        "        Path to the directory containing the stream data.\n",
        "    labels : list of str\n",
        "        List of labels to use for the regression.\n",
        "    num_datasets : int, optional\n",
        "        Number of datasets to read in. Default is 1.\n",
        "    \"\"\"\n",
        "\n",
        "    raw = []\n",
        "\n",
        "    for i in range(num_datasets):\n",
        "        label_fn = os.path.join(data_dir, f'labels.{i}.csv')\n",
        "        data_fn = os.path.join(data_dir, f'data.{i}.hdf5')\n",
        "\n",
        "        if os.path.exists(label_fn) & os.path.exists(data_fn):\n",
        "            print('Reading in data from {}'.format(data_fn))\n",
        "        else:\n",
        "            print('Dataset {} not found. Skipping...'.format(i))\n",
        "            continue\n",
        "\n",
        "        # read in the data and label\n",
        "        table = pd.read_csv(label_fn)\n",
        "        data, ptr = io_utils.read_dataset(data_fn, unpack=True)\n",
        "\n",
        "        # compute some derived labels\n",
        "        table = datasets.calculate_derived_properties(table)\n",
        "\n",
        "        loop = tqdm(range(len(table)))\n",
        "\n",
        "        for pid in loop:\n",
        "            loop.set_description(f'Processing pid {pid}')\n",
        "            phi1 = data['phi1'][pid]\n",
        "            phi2 = data['phi2'][pid]\n",
        "            pm1 = data['pm1'][pid]\n",
        "            pm2 = data['pm2'][pid]\n",
        "            vr = data['vr'][pid]\n",
        "            dist = data['dist'][pid]\n",
        "\n",
        "            raw.append(np.stack([phi1, phi2, pm1, pm2, vr, dist]))\n",
        "\n",
        "    return raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vimpQFzpVQas"
      },
      "source": [
        "Now we can get the raw data and binned data.\n",
        "\n",
        "\n",
        "```\n",
        "'x: (binned data) phi2, pm1, pm2, vr, distance'\n",
        "'t: (binned data) phi1'\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXNhC9rV1cEl"
      },
      "outputs": [],
      "source": [
        "raw = datasets.read_raw_dataset(raw_data_dir, data_labels)\n",
        "x, y, t, padding_mask = datasets.read_process_dataset(raw_data_dir, data_labels, num_bins, phi1_min=phi1_min, phi1_max=phi1_max, frac=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1Cy8_imV1Eh"
      },
      "source": [
        "We can change the index number to analyze different streams. Here we pick `stream 512 `as an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07e3orjv4BbY"
      },
      "outputs": [],
      "source": [
        "index = 512\n",
        "\n",
        "'particle'\n",
        "r_phi1 = raw[index][0]\n",
        "r_phi2 = raw[index][1]\n",
        "r_pm1 = raw[index][2]\n",
        "r_pm2 = raw[index][3]\n",
        "r_vr = raw[index][4]\n",
        "r_dist = raw[index][5]\n",
        "\n",
        "'binned'\n",
        "phi1s = t[index]\n",
        "\n",
        "phi2s = x[index][:,0]\n",
        "pm1s = x[index][:,1]\n",
        "pm2s = x[index][:,2]\n",
        "vrs = x[index][:,3]\n",
        "dist = x[index][:,4]\n",
        "\n",
        "phi2_std = x[index][:,5]\n",
        "pm1_std = x[index][:,6]\n",
        "pm2_std = x[index][:,7]\n",
        "vr_std = x[index][:,8]\n",
        "dist_std = x[index][:,9]\n",
        "\n",
        "fraction = x[index][:, 10]\n",
        "\n",
        "'filter out empty bins'\n",
        "mask = phi2_std != 0\n",
        "\n",
        "phi1s = phi1s[mask]\n",
        "\n",
        "phi2s = phi2s[mask]\n",
        "pm1s = pm1s[mask]\n",
        "pm2s = pm2s[mask]\n",
        "vrs = vrs[mask]\n",
        "dist = dist[mask]\n",
        "\n",
        "phi2_std = phi2_std[mask]\n",
        "pm1_std = pm1_std[mask]\n",
        "pm2_std = pm2_std[mask]\n",
        "vr_std = vr_std[mask]\n",
        "dist_std = dist_std[mask]\n",
        "\n",
        "fraction = fraction[mask]\n",
        "\n",
        "'filter out out-of-range bins, and sort by phi1 values'\n",
        "'These are useful for calculating spline fit'\n",
        "mask2 = (r_phi1 >= phi1_min) & (r_phi1 <= phi1_max)\n",
        "sorted_phi1 = r_phi1[mask2]\n",
        "sorted_phi2 = r_phi2[mask2]\n",
        "sorted_pm1 = r_pm1[mask2]\n",
        "sorted_pm2 = r_pm2[mask2]\n",
        "sorted_vr = r_vr[mask2]\n",
        "sorted_dist = r_dist[mask2]\n",
        "\n",
        "sorted_indices = np.argsort(r_phi1)\n",
        "sorted_phi1 = sorted_phi1[sorted_indices]\n",
        "sorted_phi2 = sorted_phi2[sorted_indices]\n",
        "sorted_pm1 = sorted_pm1[sorted_indices]\n",
        "sorted_pm2 = sorted_pm2[sorted_indices]\n",
        "sorted_vr = sorted_vr[sorted_indices]\n",
        "sorted_dist = sorted_dist[sorted_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv_QZZnI-zYd"
      },
      "source": [
        "### **example**: analyze phi1 vs phi2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OulwL5ZgD00K"
      },
      "source": [
        "We calculate the spline fit for `phi1` vs `phi2`. `knots` is a list of locations for internal knots of spline. We pick `bin_centers` as locations for `knots` ***excluding some first and last bins*** (sometimes they happen to violate the *Schoenberg-Whitney Conditions* required for `LSQUnivariateSpline`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDRBqzp072XM"
      },
      "outputs": [],
      "source": [
        "xy = np.transpose(np.vstack((sorted_phi1, sorted_phi2)))\n",
        "knots = np.sort(phi1s[:, 0])\n",
        "knots = knots[1:-2]\n",
        "splined = LSQUnivariateSpline(sorted_phi1, sorted_phi2, knots)\n",
        "y_splined = splined(sorted_phi1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejUWoi9tG7Rn"
      },
      "source": [
        "We now can visualize this stream, with particles, bin data, and spline fit. Notice that `splined` is a function that takes in as list of `x_coordinate` and spits out the predicted `y_coordinate` on spline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXEGuusw-n91"
      },
      "outputs": [],
      "source": [
        "'plot stream'\n",
        "plt.xlabel('phi1')\n",
        "plt.ylabel('phi2')\n",
        "\n",
        "#plot bin\n",
        "plt.errorbar(phi1s, phi2s, yerr=phi2_std, color='black',fmt='o', ecolor='black')\n",
        "\n",
        "#plot particle\n",
        "plt.scatter(sorted_phi1, sorted_phi2, color='#FFB9B8', s=0.2)\n",
        "\n",
        "#plot spline\n",
        "plt.scatter(sorted_phi1, splined(sorted_phi1), color='red', s=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UN60nL4HJej"
      },
      "source": [
        "We also want to analyze how much the stream particles parameter values deviate from the spline fit, along the `phi1` coordinate. Here we pick number of bins to be 20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WW8qsF2c5szO"
      },
      "outputs": [],
      "source": [
        "'plot deviation from spline'\n",
        "y_diff = sorted_phi2 - y_splined\n",
        "spline_mean, bin_edges, _ = stats.binned_statistic(sorted_phi1, y_diff, statistic='mean', bins=20)\n",
        "spline_std, _, _ = stats.binned_statistic(sorted_phi1, y_diff, statistic='std', bins=20)\n",
        "spline_count, _, _= stats.binned_statistic(sorted_phi1, y_diff, statistic='count', bins=20)\n",
        "bin_centers = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
        "\n",
        "plt.errorbar(bin_centers, spline_mean, yerr=spline_std, color='black',fmt='o', ecolor='black')\n",
        "plt.scatter(sorted_phi1, y_diff, color='#FFB9B8', s=0.2)\n",
        "plt.xlabel(r'$\\phi_{1}$')\n",
        "plt.ylabel(r'$\\delta \\phi_{2}$')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg2O9UKRCUaF"
      },
      "source": [
        "# Analyze the output\n",
        "in general, we want to make the following plots:\n",
        "\n",
        "1. corner plot for a particular stream\n",
        "2. coverage plot for the posteriors\n",
        "3. predicted posteriors vs. truths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3v0mL3KC6OK"
      },
      "source": [
        "### **example**: msat, vz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg2BXKGtKeZ_"
      },
      "source": [
        "First we need to pick which regression model we want to analyze. `run_name` and its detailed info are kept in this [google sheet](https://docs.google.com/spreadsheets/d/1R_vq7Ol2kOJ5HsCiQlfRdyKBZtF_8E-PQdUIktq2foM/edit?usp=sharing). Note that `seed` may vary depending on the run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZJU0pTK00rs"
      },
      "outputs": [],
      "source": [
        "#get trained model\n",
        "checkpoint_root = '/global/homes/r/rutong/logging/'\n",
        "run_name = 'sleepy-onion-76'\n",
        "\n",
        "checkpoint_path = f\"{checkpoint_root}\" \\\n",
        "    f\"{run_name}/lightning_logs/checkpoints/epoch=562-step=30965.ckpt\"\n",
        "\n",
        "model = regressor.Regressor.load_from_checkpoint(checkpoint_path)\n",
        "\n",
        "seed = 11\n",
        "pl.seed_everything(seed)\n",
        "\n",
        "# read in the dataset and prepare the data loader for training\n",
        "data_root = '/pscratch/sd/r/rutong/stream_sbi/datasets/2params-n1000'\n",
        "data_name = '/2params-n1000'\n",
        "data_processed_path = os.path.join(data_root, f\"processed/{run_name}.pkl\")\n",
        "\n",
        "with open(data_processed_path, \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "train_loader, val_loader, norm_dict = datasets.prepare_dataloader(\n",
        "    data, train_frac=0.8, train_batch_size=1024, eval_batch_size=128,\n",
        "    num_workers=4, seed=seed\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef2pIUwAK9ou"
      },
      "source": [
        "Here we draw `5000` samples from this model.\n",
        "`labels` indicate which parameters we used for regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyFlo3u2DJXT"
      },
      "outputs": [],
      "source": [
        "samples, labels = infer_utils.sample(model, val_loader, 5000, norm_dict=norm_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygS_2jqBCXOQ"
      },
      "outputs": [],
      "source": [
        "#corner plot\n",
        "fig = corner.corner(\n",
        "    samples[index],\n",
        "    truths=labels[index],\n",
        "    truth_color='#FF5E6D',\n",
        "    levels=(0.68, 0.95),\n",
        "    show_titles=True,\n",
        "    labels=[r'$\\log M_\\mathrm{sat}$', r'$v_z$'],\n",
        "    title_kwargs={\"fontsize\": 12},\n",
        "    plot_contours=True,\n",
        "    color='#3B23AB',\n",
        "    hist_bin_factor = 3\n",
        ")\n",
        "\n",
        "plt.rcParams['figure.dpi'] = 500\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuZMrwzQX4m7"
      },
      "source": [
        "These two functions help find the confidence interval of posteriors, and calcualte the coverage given truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OZ4Gt7Qz9fF"
      },
      "outputs": [],
      "source": [
        "def find_confidence_inverval(posteriors, confidence_level):\n",
        "    lower_percent = (100 - confidence_level) / 2\n",
        "    lower = np.percentile(posteriors, lower_percent, axis=1)\n",
        "    upper_percent = 100 - (100 - confidence_level) / 2\n",
        "    upper = np.percentile(posteriors, upper_percent, axis=1)\n",
        "    return (lower, upper)\n",
        "\n",
        "def calculate_coverage(truths, posteriors, confidence_level_list):\n",
        "    '''\n",
        "    Return coverage of the given posteriors\n",
        "    '''\n",
        "    coverage = list()\n",
        "    for cl in confidence_level_list:\n",
        "        lower, upper = find_confidence_inverval(posteriors, cl)\n",
        "        percent = np.mean((lower < truths) & (truths < upper))\n",
        "        coverage.append(percent)\n",
        "    coverage = np.array(coverage)\n",
        "    return coverage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1FXW4w06v1E"
      },
      "source": [
        "For example, if we want to make a coverage plot for vz and msat:\n",
        "\n",
        "\n",
        "1.   specify a list of level of confidence:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ew5IsRCcCliD"
      },
      "outputs": [],
      "source": [
        "confidence_level_list = np.linspace(5, 95, 19)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81LupMpoXahJ"
      },
      "source": [
        "2.   calculate the corresponding coverage for each parameter.\n",
        "notice that `labels` is a list of string represents the parameters we used for regression; `samples` is an array of samples we draw from the model, where `samples[0]` is the first stream in the sample, and `samples[0][0]` returns the `msat` of `stream_0`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jTltirJXdl6"
      },
      "outputs": [],
      "source": [
        "coverages = []\n",
        "for i in range(2):\n",
        "     coverage = calculate_coverage(labels[..., i], samples[..., i], confidence_level_list)\n",
        "     coverages.append(coverage)\n",
        "\n",
        "msat, vz = np.array(coverages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkgyTMuzXgJD"
      },
      "source": [
        "3. coverage plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2yHfxlzXicM"
      },
      "outputs": [],
      "source": [
        "plt.plot([(0, 0), (1, 1)], color='black', linestyle='--', linewidth=2)\n",
        "plt.plot(confidence_level_list/100, msat, label=r'$\\log M_\\mathrm{sat}$', color = '#00CFFF')\n",
        "plt.plot(confidence_level_list/100, vz, label=r'$v_z$', color = '#D30000')\n",
        "plt.xlabel('confidence level')\n",
        "plt.ylabel('coverage')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hj2R7l5-lWe"
      },
      "source": [
        "This following function helps calculate and plot the predicted posteriors vs. truths. For example, we use medians of the predicted posteriors `msat`, `vz` given by model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MupobhefzqEE"
      },
      "outputs": [],
      "source": [
        "def predicted_vs_true(p, t, num_bins=10):\n",
        "    '''calcaulate predicted vs true posterior,\n",
        "    return bin_centers, median, error, standard deviation\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    p: a list of predicted posteriors\n",
        "    t: a list of true posteriors\n",
        "    num_bins: number of bins on t\n",
        "    '''\n",
        "    bin_edges = np.linspace(t.min(), t.max(), num_bins+1)\n",
        "    bin_centers = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
        "    p_median = np.zeros(num_bins)\n",
        "    p_stdv = np.zeros(num_bins)\n",
        "    p_count = np.zeros(num_bins)\n",
        "    for i in range(num_bins):\n",
        "        mask = (t >= bin_edges[i]) & (t <= bin_edges[i + 1])\n",
        "        if mask.sum() <= 1:\n",
        "            continue\n",
        "        p_median[i] = p[mask].mean(axis=0)\n",
        "        p_stdv[i] = p[mask].std(axis=0)\n",
        "        p_count[i] = mask.sum()\n",
        "    p_error = p_stdv / (p_count)**0.5\n",
        "    return bin_centers, p_median, p_error, p_stdv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-wWbPxBC1Wj"
      },
      "outputs": [],
      "source": [
        "# plot predicted values (median) vs true values\n",
        "def find_posterior_medians(posteriors):\n",
        "     return [np.median(posterior) for posterior in posteriors]\n",
        "\n",
        "predicted = []\n",
        "for i in range(2):\n",
        "     predicted.append(find_posterior_medians(samples[..., i]))\n",
        "\n",
        "msat_predicted, vz_predicted = np.array(predicted)\n",
        "msat_true = labels[..., 0]\n",
        "vz_true = labels[..., 1]\n",
        "\n",
        "'Notice that num_bins here is not the same as num_bins in model. You can vary this value to make better plots.'\n",
        "msat_centers, msat_median, msat_error, msat_stdv = predicted_vs_true(msat_predicted, msat_true, num_bins=30)\n",
        "vz_centers, vz_median, vz_error, vz_stdv = predicted_vs_true(vz_predicted, vz_true, num_bins=30)\n",
        "\n",
        "'msat'\n",
        "plt.scatter(msat_true, msat_predicted, s=2, color='#C9D8D1')\n",
        "plt.errorbar(msat_centers, msat_median, yerr=msat_error, fmt='o', color='red', markersize=1, ecolor='black', capsize=5)\n",
        "plt.title(r'$\\log M_\\mathrm{sat}$')\n",
        "plt.xlabel('true')\n",
        "plt.ylabel('predicted')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "'vz'\n",
        "plt.scatter(vz_true, vz_predicted, s=2, color='#C9D8D1')\n",
        "plt.errorbar(vz_centers, vz_median, yerr=vz_error, fmt='o', color='red', markersize=1, ecolor='black', capsize=5)\n",
        "plt.title(r'$v_z$')\n",
        "plt.xlabel('true')\n",
        "plt.ylabel('predicted')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
